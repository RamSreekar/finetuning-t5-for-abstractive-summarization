{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\n\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW, PreTrainedTokenizer\n# from transformers import PegasusForConditionalGeneration, PegasusTokenizer\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:17:24.415328Z","iopub.execute_input":"2023-05-04T06:17:24.416303Z","iopub.status.idle":"2023-05-04T06:17:36.158202Z","shell.execute_reply.started":"2023-05-04T06:17:24.416261Z","shell.execute_reply":"2023-05-04T06:17:36.156950Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\ntorch.backends.cuda.max_split_size_mb = 10\n\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-05-04T06:17:36.160721Z","iopub.execute_input":"2023-05-04T06:17:36.161902Z","iopub.status.idle":"2023-05-04T06:17:36.236786Z","shell.execute_reply.started":"2023-05-04T06:17:36.161851Z","shell.execute_reply":"2023-05-04T06:17:36.235527Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Loading the tokenizer and dataset","metadata":{}},{"cell_type":"code","source":"model_name = 't5-large'\ntokenizer = T5Tokenizer.from_pretrained(model_name)\n#t5_model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:17:36.240104Z","iopub.execute_input":"2023-05-04T06:17:36.240593Z","iopub.status.idle":"2023-05-04T06:17:37.099223Z","shell.execute_reply.started":"2023-05-04T06:17:36.240561Z","shell.execute_reply":"2023-05-04T06:17:37.097997Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fecdc228e6f4d65aba01c328eb45eb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63131e7f4ccd403d8f51f8b480a2fa3c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/models/t5/tokenization_t5.py:173: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\nFor now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"#dataset = pd.read_csv('/kaggle/input/dataset-summarization/summarization_dataset_final.csv')\ndataset = pd.read_csv('/kaggle/input/data-final/summarization_data_final.csv')","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:17:37.102163Z","iopub.execute_input":"2023-05-04T06:17:37.102634Z","iopub.status.idle":"2023-05-04T06:17:37.134487Z","shell.execute_reply.started":"2023-05-04T06:17:37.102572Z","shell.execute_reply":"2023-05-04T06:17:37.133488Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(dataset)\ndf = df.loc[:, [\"text\", \"summary\"]]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:17:37.136703Z","iopub.execute_input":"2023-05-04T06:17:37.137485Z","iopub.status.idle":"2023-05-04T06:17:37.160146Z","shell.execute_reply.started":"2023-05-04T06:17:37.137444Z","shell.execute_reply":"2023-05-04T06:17:37.158348Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                                text  \\\n0  artificial intelligence is transforming the he...   \n1  the future of transportation is electric with ...   \n2  remote work is here to stay and its changing t...   \n3  blockchain technology is transforming the way ...   \n4  the global food system is facing unprecedented...   \n\n                                             summary  \n0  ai is revolutionizing healthcare by improving ...  \n1  evs are the future of transportation and as we...  \n2  remote work is changing the way we balance wor...  \n3  blockchain is transforming business by enablin...  \n4  building a sustainable and equitable food syst...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>artificial intelligence is transforming the he...</td>\n      <td>ai is revolutionizing healthcare by improving ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>the future of transportation is electric with ...</td>\n      <td>evs are the future of transportation and as we...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>remote work is here to stay and its changing t...</td>\n      <td>remote work is changing the way we balance wor...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>blockchain technology is transforming the way ...</td>\n      <td>blockchain is transforming business by enablin...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>the global food system is facing unprecedented...</td>\n      <td>building a sustainable and equitable food syst...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['text'] = 'summarize: ' + df['text'].astype(str)\n\ndf.rename(columns = {'text':'source_text', 'summary':'target_text'}, inplace = True)\ndf.columns","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:17:37.161908Z","iopub.execute_input":"2023-05-04T06:17:37.162459Z","iopub.status.idle":"2023-05-04T06:17:37.175831Z","shell.execute_reply.started":"2023-05-04T06:17:37.162419Z","shell.execute_reply":"2023-05-04T06:17:37.173778Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Index(['source_text', 'target_text'], dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:17:37.177490Z","iopub.execute_input":"2023-05-04T06:17:37.177977Z","iopub.status.idle":"2023-05-04T06:17:37.191523Z","shell.execute_reply.started":"2023-05-04T06:17:37.177935Z","shell.execute_reply":"2023-05-04T06:17:37.189927Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                         source_text  \\\n0  summarize: artificial intelligence is transfor...   \n1  summarize: the future of transportation is ele...   \n2  summarize: remote work is here to stay and its...   \n3  summarize: blockchain technology is transformi...   \n4  summarize: the global food system is facing un...   \n\n                                         target_text  \n0  ai is revolutionizing healthcare by improving ...  \n1  evs are the future of transportation and as we...  \n2  remote work is changing the way we balance wor...  \n3  blockchain is transforming business by enablin...  \n4  building a sustainable and equitable food syst...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source_text</th>\n      <th>target_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>summarize: artificial intelligence is transfor...</td>\n      <td>ai is revolutionizing healthcare by improving ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>summarize: the future of transportation is ele...</td>\n      <td>evs are the future of transportation and as we...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>summarize: remote work is here to stay and its...</td>\n      <td>remote work is changing the way we balance wor...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>summarize: blockchain technology is transformi...</td>\n      <td>blockchain is transforming business by enablin...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>summarize: the global food system is facing un...</td>\n      <td>building a sustainable and equitable food syst...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:17:37.193323Z","iopub.execute_input":"2023-05-04T06:17:37.193807Z","iopub.status.idle":"2023-05-04T06:17:37.201585Z","shell.execute_reply.started":"2023-05-04T06:17:37.193766Z","shell.execute_reply":"2023-05-04T06:17:37.200072Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(1351, 2)"},"metadata":{}}]},{"cell_type":"code","source":"test_df = df.iloc[1201:, :]","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:17:37.203375Z","iopub.execute_input":"2023-05-04T06:17:37.203889Z","iopub.status.idle":"2023-05-04T06:17:37.211169Z","shell.execute_reply.started":"2023-05-04T06:17:37.203847Z","shell.execute_reply":"2023-05-04T06:17:37.209716Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df = df.iloc[:1201, :]","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:17:37.217519Z","iopub.execute_input":"2023-05-04T06:17:37.218637Z","iopub.status.idle":"2023-05-04T06:17:37.224185Z","shell.execute_reply.started":"2023-05-04T06:17:37.218592Z","shell.execute_reply":"2023-05-04T06:17:37.222862Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Custom T5 Model\n#### Defining custom T5 model with an additional layer","metadata":{}},{"cell_type":"code","source":"class CustomT5Model(nn.Module):\n    def __init__(self, model_name):\n        super(CustomT5Model, self).__init__()\n        self.model_name = model_name\n        print(\"model_name : \"+self.model_name+\"\\n\")\n        self.model = T5ForConditionalGeneration.from_pretrained(self.model_name)\n        #Remove the last layer\n        self.encoder_blocks = nn.ModuleList(self.model.encoder.block[:-1])\n        #Additional layer added on top of the existing model\n        self.linear = nn.Linear(self.model.config.hidden_size, self.model.config.hidden_size)\n        self.relu = nn.ReLU()\n    \n    def forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask):\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask,\n                             decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask)\n        hidden_states = outputs.encoder_last_hidden_state\n        new_hidden_states = self.linear(hidden_states)\n        new_hidden_states = self.relu(new_hidden_states)\n        outputs.encoder_last_hidden_state = new_hidden_states\n        return outputs\n    \n    def generate_summary(self, input_text):\n        input_ids = tokenizer.encode(input_text, return_tensors='pt')\n        input_ids = input_ids.to(device)\n        generated_ids = self.model.generate(input_ids=input_ids)\n        generated_ids = generated_ids.to(device)\n        summary = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n        return summary","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:17:37.226185Z","iopub.execute_input":"2023-05-04T06:17:37.226901Z","iopub.status.idle":"2023-05-04T06:17:37.239499Z","shell.execute_reply.started":"2023-05-04T06:17:37.226856Z","shell.execute_reply":"2023-05-04T06:17:37.238049Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Custom PandasDataset\n#### An instance of this class takes the pandas dataframe and returns some required tensors ","metadata":{}},{"cell_type":"code","source":"class PandasDataset(Dataset):\n    def __init__(\n        self,\n        data: pd.DataFrame,\n        tokenizer: PreTrainedTokenizer,\n        source_max_token_len: int = 512,\n        target_max_token_len: int = 128,\n    ):\n        self.tokenizer = tokenizer\n        self.data = data\n        self.source_max_token_len = source_max_token_len\n        self.target_max_token_len = target_max_token_len\n\n    def __len__(self):\n        \"\"\" returns length of data \"\"\"\n        return len(self.data)\n\n    def __getitem__(self, index: int):\n        \"\"\" returns dictionary of input tensors to feed into T5 model\"\"\"\n\n        data_row = self.data.iloc[index]\n        source_text = data_row[\"source_text\"]\n\n        source_text_encoding = self.tokenizer(\n            source_text,\n            max_length=self.source_max_token_len,\n            padding=True,\n            truncation=True,\n            return_attention_mask=True,\n            add_special_tokens=True,\n            return_tensors=\"pt\",\n        )\n\n        target_text_encoding = self.tokenizer(\n            data_row[\"target_text\"],\n            max_length=self.target_max_token_len,\n            padding=True,\n            truncation=True,\n            return_attention_mask=True,\n            add_special_tokens=True,\n            return_tensors=\"pt\",\n        )\n\n        labels = target_text_encoding[\"input_ids\"]\n        labels[\n            labels == 0\n        ] = -100  \n\n        return dict(\n            input_ids=torch.tensor(source_text_encoding[\"input_ids\"].flatten()),\n            attention_mask=torch.tensor(source_text_encoding[\"attention_mask\"].flatten()),\n            labels=labels.flatten(),\n            decoder_input_ids = torch.tensor(target_text_encoding[\"input_ids\"].flatten()),\n            decoder_attention_mask=torch.tensor(target_text_encoding[\"attention_mask\"].flatten())\n        )","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:17:37.241756Z","iopub.execute_input":"2023-05-04T06:17:37.244819Z","iopub.status.idle":"2023-05-04T06:17:37.259111Z","shell.execute_reply.started":"2023-05-04T06:17:37.244693Z","shell.execute_reply":"2023-05-04T06:17:37.257874Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Collate function for the data loader\n#### Used for padding the sequences to the same length","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    input_ids = [item[\"input_ids\"] for item in batch]\n    attention_mask = [item[\"attention_mask\"] for item in batch]\n    target_ids = [item[\"labels\"] for item in batch]\n    decoder_input_ids = [item[\"decoder_input_ids\"] for item in batch]\n    decoder_attention_mask = [item[\"decoder_attention_mask\"] for item in batch]\n\n    \n    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n    target_ids = torch.nn.utils.rnn.pad_sequence(target_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n    decoder_input_ids = torch.nn.utils.rnn.pad_sequence(decoder_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n    decoder_attention_mask = torch.nn.utils.rnn.pad_sequence(decoder_attention_mask, batch_first=True, padding_value=tokenizer.pad_token_id)\n\n    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": target_ids, \"decoder_input_ids\": decoder_input_ids, \"decoder_attention_mask\": decoder_attention_mask}\n","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:17:37.261098Z","iopub.execute_input":"2023-05-04T06:17:37.261757Z","iopub.status.idle":"2023-05-04T06:17:37.274631Z","shell.execute_reply.started":"2023-05-04T06:17:37.261684Z","shell.execute_reply":"2023-05-04T06:17:37.273582Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Train, Validation Split and Creating DataLoaders","metadata":{}},{"cell_type":"code","source":"# Create the training and validation datasets\ntrain_df = df.sample(frac=0.75, random_state=45)\nvalid_df = df.drop(train_df.index)\n\n# Convert the datasets to required format using the custom PandasDataset class\ntrain_dataset = PandasDataset(train_df, tokenizer)\nvalid_dataset = PandasDataset(valid_df, tokenizer)\n\n# Create the data loaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:17:37.276116Z","iopub.execute_input":"2023-05-04T06:17:37.277061Z","iopub.status.idle":"2023-05-04T06:17:37.293260Z","shell.execute_reply.started":"2023-05-04T06:17:37.276993Z","shell.execute_reply":"2023-05-04T06:17:37.292247Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"# Instatntiate the model\nmodel = CustomT5Model(model_name)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:17:37.294824Z","iopub.execute_input":"2023-05-04T06:17:37.295594Z","iopub.status.idle":"2023-05-04T06:19:21.634255Z","shell.execute_reply.started":"2023-05-04T06:17:37.295551Z","shell.execute_reply":"2023-05-04T06:19:21.633093Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"model_name : t5-large\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/2.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d526b13a9653470492e42105308af108"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3df168173b4459eafaa3e4498ef37df"}},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"CustomT5Model(\n  (model): T5ForConditionalGeneration(\n    (shared): Embedding(32128, 1024)\n    (encoder): T5Stack(\n      (embed_tokens): Embedding(32128, 1024)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n                (relative_attention_bias): Embedding(32, 16)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (2): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (3): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (4): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (5): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (6): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (7): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (8): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (9): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (10): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (11): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (12): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (13): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (14): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (15): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (16): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (17): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (18): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (19): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (20): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (21): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (22): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (23): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (decoder): T5Stack(\n      (embed_tokens): Embedding(32128, 1024)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n                (relative_attention_bias): Embedding(32, 16)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (2): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (3): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (4): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (5): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (6): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (7): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (8): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (9): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (10): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (11): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (12): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (13): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (14): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (15): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (16): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (17): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (18): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (19): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (20): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (21): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (22): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (23): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n  )\n  (encoder_blocks): ModuleList(\n    (0): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n            (relative_attention_bias): Embedding(32, 16)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (1): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (2): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (3): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (4): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (5): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (6): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (7): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (8): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (9): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (10): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (11): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (12): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (13): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (14): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (15): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (16): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (17): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (18): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (19): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (20): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (21): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (22): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (linear): Linear(in_features=1024, out_features=1024, bias=True)\n  (relu): ReLU()\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Defining the training arguments\ntraining_args = {\"num_train_epochs\": 10, \"learning_rate\": 1e-5}\n\n# Defining the optimizer and loss function\noptimizer = AdamW(model.parameters(), lr=training_args[\"learning_rate\"])\nloss_fn = torch.nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:19:21.635459Z","iopub.execute_input":"2023-05-04T06:19:21.636880Z","iopub.status.idle":"2023-05-04T06:19:21.652936Z","shell.execute_reply.started":"2023-05-04T06:19:21.636835Z","shell.execute_reply":"2023-05-04T06:19:21.651527Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"validation_losses = []\n\nfor epoch in range(training_args[\"num_train_epochs\"]):\n    # Train the model on the training dataset\n    model.train()\n\n    for batch in train_loader:\n        # Get the input and target summaries\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(device)\n        target_ids = batch[\"labels\"].to(device)\n        decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n\n        # Compute the model's predictions\n        outputs = model(input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, decoder_attention_mask = decoder_attention_mask)\n            \n        # Compute the loss between the predictions and the target summaries\n        loss = loss_fn(outputs.logits.view(-1, outputs.logits.shape[-1]), decoder_input_ids.view(-1))\n            \n        # Backpropagate the loss and update the model parameters\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n    # Evaluate the model on the validation dataset\n    model.eval()\n    with torch.no_grad():\n        total_loss = 0\n        for batch in valid_loader:\n            # Get the input and target summaries\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n            decoder_attention_mask = batch[\"decoder_attention_mask\"].to(device)\n            target_ids = batch[\"labels\"].to(device)\n            \n            #outputs = model(input_ids, attention_mask=attention_mask, decoder_input_ids=target_ids[:, :-1], decoder_attention_mask = decoder_attention_mask)\n            \n            outputs = model(input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, decoder_attention_mask = decoder_attention_mask)\n            \n            loss = loss_fn(outputs.logits.view(-1, outputs.logits.shape[-1]), decoder_input_ids.view(-1))\n            \n            total_loss += loss.item()\n\n    # Calculate the average validation loss and print the results\n    avg_val_loss = total_loss / len(valid_loader)\n    validation_losses.append(avg_val_loss)\n    \n    print(\"Epoch [{}/{}] : Validation Loss: {:.4f}\".format(epoch+1, training_args[\"num_train_epochs\"], avg_val_loss))","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:19:21.655037Z","iopub.execute_input":"2023-05-04T06:19:21.655673Z","iopub.status.idle":"2023-05-04T06:42:43.269144Z","shell.execute_reply.started":"2023-05-04T06:19:21.655616Z","shell.execute_reply":"2023-05-04T06:42:43.267755Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10] : Validation Loss: 4.5086\nEpoch [2/10] : Validation Loss: 3.4192\nEpoch [3/10] : Validation Loss: 2.8043\nEpoch [4/10] : Validation Loss: 2.4507\nEpoch [5/10] : Validation Loss: 2.0898\nEpoch [6/10] : Validation Loss: 1.7755\nEpoch [7/10] : Validation Loss: 1.5298\nEpoch [8/10] : Validation Loss: 1.2650\nEpoch [9/10] : Validation Loss: 1.0283\nEpoch [10/10] : Validation Loss: 0.8356\n","output_type":"stream"}]},{"cell_type":"code","source":"validation_losses","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:42:43.270638Z","iopub.execute_input":"2023-05-04T06:42:43.271319Z","iopub.status.idle":"2023-05-04T06:42:43.279452Z","shell.execute_reply.started":"2023-05-04T06:42:43.271274Z","shell.execute_reply":"2023-05-04T06:42:43.278150Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[4.508616844813029,\n 3.419220390319824,\n 2.8042695776621502,\n 2.4506647459665936,\n 2.08981858253479,\n 1.7754628149668377,\n 1.5298491032918293,\n 1.2649908725420633,\n 1.0282689285278321,\n 0.8355542341868083]"},"metadata":{}}]},{"cell_type":"markdown","source":"## Validation Loss vs Epoch Curve ","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nplt.plot([i for i in range(1, len(validation_losses)+1)], validation_losses)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:42:43.281272Z","iopub.execute_input":"2023-05-04T06:42:43.282656Z","iopub.status.idle":"2023-05-04T06:42:43.546865Z","shell.execute_reply.started":"2023-05-04T06:42:43.282554Z","shell.execute_reply":"2023-05-04T06:42:43.545726Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7ce1180c5290>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model-large.pt')","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:42:43.548941Z","iopub.execute_input":"2023-05-04T06:42:43.550065Z","iopub.status.idle":"2023-05-04T06:42:49.601457Z","shell.execute_reply.started":"2023-05-04T06:42:43.549981Z","shell.execute_reply":"2023-05-04T06:42:49.600254Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Generating summaries from the finetuned model","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load('model-large.pt'))","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:42:49.603084Z","iopub.execute_input":"2023-05-04T06:42:49.603498Z","iopub.status.idle":"2023-05-04T06:42:51.777951Z","shell.execute_reply.started":"2023-05-04T06:42:49.603451Z","shell.execute_reply":"2023-05-04T06:42:51.776960Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"input_text = \"summarize: the covid-19 vaccine is approved, as appointments ramp up, you’ll be able to find a vaccine at pediatricians’ offices, pharmacies, and more\"\n\noutput_text = model.generate_summary(input_text)\n\nprint(\"Original text :\\n\"+input_text)\nprint(\"\\nGenerated Summary :\\n\"+output_text)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:50:53.688335Z","iopub.execute_input":"2023-05-04T06:50:53.688725Z","iopub.status.idle":"2023-05-04T06:50:54.518117Z","shell.execute_reply.started":"2023-05-04T06:50:53.688692Z","shell.execute_reply":"2023-05-04T06:50:54.516841Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Original text :\nsummarize: the covid-19 vaccine is approved, as appointments ramp up, you’ll be able to find a vaccine at pediatricians’ offices, pharmacies, and more\n\nGenerated Summary :\nthe covid-19 vaccine is approved and will be available at pediatricians’ offices, pharmacies\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install rouge","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:42:54.181433Z","iopub.execute_input":"2023-05-04T06:42:54.182112Z","iopub.status.idle":"2023-05-04T06:43:06.918860Z","shell.execute_reply.started":"2023-05-04T06:42:54.182067Z","shell.execute_reply":"2023-05-04T06:43:06.917396Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Collecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from rouge) (1.16.0)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from rouge import Rouge\n\n# Initialize the ROUGE evaluator\nrouge = Rouge()\n\n# Calculate the ROUGE scores\nscores = rouge.get_scores(input_text, output_text, avg=True)\nprint(scores)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:43:06.921755Z","iopub.execute_input":"2023-05-04T06:43:06.922242Z","iopub.status.idle":"2023-05-04T06:43:06.952466Z","shell.execute_reply.started":"2023-05-04T06:43:06.922184Z","shell.execute_reply":"2023-05-04T06:43:06.951250Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"{'rouge-1': {'r': 0.6428571428571429, 'p': 0.34615384615384615, 'f': 0.44999999545}, 'rouge-2': {'r': 0.5384615384615384, 'p': 0.2692307692307692, 'f': 0.35897435452991455}, 'rouge-l': {'r': 0.6428571428571429, 'p': 0.34615384615384615, 'f': 0.44999999545}}\n","output_type":"stream"}]},{"cell_type":"code","source":"generated_summaries = []\nfor document in test_df.iloc[:, 0]:\n    generated_summaries.append(model.generate_summary(document))\n    \ntarget_summaries = []\nfor document in test_df.iloc[:, 1]:\n    target_summaries.append(document)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:43:06.954059Z","iopub.execute_input":"2023-05-04T06:43:06.954554Z","iopub.status.idle":"2023-05-04T06:45:13.818687Z","shell.execute_reply.started":"2023-05-04T06:43:06.954496Z","shell.execute_reply":"2023-05-04T06:45:13.817466Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from rouge import Rouge\nrouge = Rouge()","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:45:13.820473Z","iopub.execute_input":"2023-05-04T06:45:13.820884Z","iopub.status.idle":"2023-05-04T06:45:13.826431Z","shell.execute_reply.started":"2023-05-04T06:45:13.820839Z","shell.execute_reply":"2023-05-04T06:45:13.825227Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"rouge_scores = rouge.get_scores(target_summaries, generated_summaries, avg=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:45:13.828159Z","iopub.execute_input":"2023-05-04T06:45:13.828890Z","iopub.status.idle":"2023-05-04T06:45:13.905781Z","shell.execute_reply.started":"2023-05-04T06:45:13.828844Z","shell.execute_reply":"2023-05-04T06:45:13.904646Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"rouge_scores","metadata":{"execution":{"iopub.status.busy":"2023-05-04T06:45:13.911927Z","iopub.execute_input":"2023-05-04T06:45:13.912294Z","iopub.status.idle":"2023-05-04T06:45:13.920609Z","shell.execute_reply.started":"2023-05-04T06:45:13.912259Z","shell.execute_reply":"2023-05-04T06:45:13.919310Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"{'rouge-1': {'r': 0.5550044008565681,\n  'p': 0.37847219722200426,\n  'f': 0.4403183742423761},\n 'rouge-2': {'r': 0.3509954865396041,\n  'p': 0.2325893004086752,\n  'f': 0.27399666620121205},\n 'rouge-l': {'r': 0.51639431758472,\n  'p': 0.35146338068693916,\n  'f': 0.4093048001974516}}"},"metadata":{}}]},{"cell_type":"markdown","source":"The best scores so far are shown below :\n\nNumber of epochs : 30\nLearning Rate : 1e-6\n\n{'rouge-1': {'r': 0.5589297466952267,\n  'p': 0.40518956992855015,\n  'f': 0.4611874515740227},\n 'rouge-2': {'r': 0.37026811151811145,\n  'p': 0.25087603810608,\n  'f': 0.2935283646665617},\n 'rouge-l': {'r': 0.5229177484881821,\n  'p': 0.3779541726583791,\n  'f': 0.4308589315342012}\n}","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}